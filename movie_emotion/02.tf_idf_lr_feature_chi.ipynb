{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T21:38:49.666960Z",
     "start_time": "2019-09-05T21:38:49.635760Z"
    }
   },
   "outputs": [],
   "source": [
    "def chi(term, cl, X, Y):\n",
    "    \"\"\" chisquare（卡方）检验算法，最常用特征选择算法之一 \"\"\"\n",
    "    N = len(Y)\n",
    "    A = B = C = D = 0\n",
    "    for i, x in enumerate(X):\n",
    "        flag = False\n",
    "        #print(\"i:%s,x:%s\" % (i,x))\n",
    "        #for k in x.keys():\n",
    "        for k in x.split(' '):\n",
    "            #print(\"k:%s,term:%s\" %(k, term))\n",
    "            if k == term:\n",
    "                flag = True\n",
    "                break\n",
    "        \n",
    "        if flag == True:\n",
    "            if str(Y[i]) == str(cl):\n",
    "                A = A + 1\n",
    "            else:\n",
    "                B = B + 1\n",
    "        else:\n",
    "            if str(Y[i]) == str(cl):\n",
    "                C = C + 1\n",
    "            else:\n",
    "                D = D + 1\n",
    "    print(\"A:%d, B:%d, C:%d, D:%d\" % (A,B,C,D))\n",
    "    re = int((N * (A * D - B * C) * (A * D - B * C)) / ((A + B) * (A + C) * (B + D) * (C + D)))\n",
    "    return re\n",
    "\n",
    "                \n",
    "def dump_feature(labels, dic, X, Y, filePath):\n",
    "    \"\"\" 遍历词袋中的词，然后与训练集数据执行卡方检验算法，并保存结果 \"\"\"\n",
    "    fp = open(filePath, 'w')\n",
    "    dic_num = len(dic)\n",
    " \n",
    "    # lable是类别列表[0, 1]\n",
    "    # 保存在txt文件中，标签为0的一行 标签为1的一行\n",
    "    for cl in labels:\n",
    "        fp.write(str(cl))\n",
    "        i = 0\n",
    "        for term in dic.keys():\n",
    "            #print(\"term:%s, c1:%s\" % (term, cl))\n",
    "            if ((i+1)%1000 == 0):\n",
    "                print(\"term %d of %d\" % (i+1, dic_num))\n",
    "            s = chi(term, cl, X, Y)\n",
    "            #print('当前检测词为: {} {}'.format(term, s))\n",
    "            fp.write(' ' + term + ':' + str(s))\n",
    "            i+=1\n",
    "            break;\n",
    "        fp.write('\\n')\n",
    "        fp.flush()\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T21:38:55.317170Z",
     "start_time": "2019-09-05T21:38:53.069766Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#impot utils\n",
    "\n",
    "TRAIN_FILE   = './data/train_data.csv'\n",
    "TEST_FILE    = './data/test_data.csv'\n",
    "SUBMIT_FILE  = './data/tfidf_lr_chi_submission.csv'\n",
    "FEATURE_FILE = './data/feature_word.txt'\n",
    "\n",
    "class TF_IDF_LR():\n",
    "    def __init__(self, input_file,  input_file_encoding, stop_words_file=False):\n",
    "        '''\n",
    "        input_file:          输入文件名\n",
    "        input_file_encoding：输入文件编码格式\n",
    "        stop_words_file：    停顿词文件名，无停顿词时设置为False\n",
    "        '''\n",
    "        self.input_file = input_file\n",
    "        self.input_file_encoding = input_file_encoding\n",
    "        self.stop_words_file = stop_words_file\n",
    "        \n",
    "    def load_data(self):\n",
    "        '''\n",
    "        利用pandas加载数据到dataframe\n",
    "        '''\n",
    "        return pd.read_csv(self.input_file, encoding=self.input_file_encoding)\n",
    "    \n",
    "    def get_data(self, test_size=0.1):\n",
    "        '''\n",
    "        取得数据的特征列和标签列，对特征列进行分词的处理，然后划分数据集为训练集和测试集\n",
    "        '''\n",
    "        df = self.load_data()\n",
    "        self.X = df['comment'].apply(self.jieba_tokenizer)\n",
    "        self.y = df['label']\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=test_size)\n",
    "\n",
    "    def dump_features(self, file_path, num_feautures):\n",
    "        vectorizer = CountVectorizer(analyzer=\"word\",\n",
    "                             tokenizer=None,\n",
    "                             preprocessor=None,\n",
    "                             stop_words=None,\n",
    "                             max_features=num_feautures)\n",
    " \n",
    "        features = vectorizer.fit_transform(self.X)\n",
    "\n",
    "        #print(\"由此可知语料库汇总词的总数量就是词袋的向量长度，且每一个词汇对应着它出现的顺序和频率\")\n",
    "        #print('feature value: ', features.toarray())\n",
    "        #print('feature name: ', features.get_feature_names())\n",
    "        #print('vectorizer.vocabulary_: ', vectorizer.vocabulary_)\n",
    "        #print('feature count: ', len(vectorizer.vocabulary_))\n",
    "        \n",
    "        labels = [0, 1]\n",
    "\n",
    "        print('开始进行卡方检测...')\n",
    "        dump_feature(labels, vectorizer.vocabulary_, self.X, self.y, filePath=file_path)\n",
    "        \n",
    "    def train(self):\n",
    "        #出现频率最高的几个，对评价无影响的词,最后未使用\n",
    "        #excludeWords = {'的','了','我','看','电影','在'}\n",
    "        excludeWords = {}\n",
    "        \n",
    "        #没有max_features:0.812344\n",
    "        #max_features = 500    #0.766281\n",
    "        #max_features = 1000   #0.772181\n",
    "        #max_features = 1500   #0.794872\n",
    "        #max_features = 2000   #0.795326\n",
    "        #max_features = 3000   #0.804402\n",
    "        #max_features = 4000   #0.804629\n",
    "        #max_features = 5000   #0.809621\n",
    "        #max_features = 6000   #0.811436\n",
    "        #max_features = 7000   #0.810302\n",
    "        #max_features = 8000   #0.811436\n",
    "        #max_features = 9000   #0.815975\n",
    "        max_features = 10000  #0.811436\n",
    "        #max_features = 11000   #0.818017\n",
    "        #max_features = 12000   #0.815067\n",
    "        #max_features = 15000    #0.812798\n",
    "        #max_features = 11000 \n",
    "        self.tfidf_vec = TfidfVectorizer(smooth_idf=1,\n",
    "                            analyzer='word',\n",
    "                            encoding='utf-8',\n",
    "                            preprocessor=None,\n",
    "                            ngram_range=(1, 2),\n",
    "                            max_features = max_features,\n",
    "                            stop_words=excludeWords)   #812798\n",
    "                                   \n",
    "    \n",
    "        # 使用tfidf的方式，将原始训练和测试文本转化为特征向量。\n",
    "        X_tfidf_train = self.tfidf_vec.fit_transform(self.X_train)\n",
    "        X_tfidf_test = self.tfidf_vec.transform(self.X_test)\n",
    "    \n",
    "        lr = LogisticRegression()\n",
    "        lr = lr.fit(X_tfidf_train, self.y_train)\n",
    "\n",
    "        print('Test set accuracy: %3f' % lr.score(X_tfidf_test, self.y_test))\n",
    "        \n",
    "        self.model = lr\n",
    "    def jieba_tokenizer(self,x):\n",
    "        '''\n",
    "        对输入进行分词处理，然后词语之间以空格分割的形式返回字符串\n",
    "        '''\n",
    "        words =  jieba.cut(x,cut_all=True)\n",
    "        return \" \".join(words)\n",
    "    \n",
    "    def predict(self, test_file, submit_file):\n",
    "        test_df = pd.read_csv(test_file,  encoding='utf-8', header=0)\n",
    "        #将行按空格分割\n",
    "        X = test_df['comment'].apply(self.jieba_tokenizer) \n",
    "        \n",
    "        #将词转换为向量\n",
    "        X_vect = self.tfidf_vec.transform(X)\n",
    "        \n",
    "        #预测\n",
    "        y_pred = self.model.predict(X_vect)\n",
    "        \n",
    "        #预测结果保存\n",
    "        res = pd.DataFrame({'label':y_pred})\n",
    "        res.to_csv(submit_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T21:39:04.774787Z",
     "start_time": "2019-09-05T21:38:58.439176Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.749 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始进行卡方检测...\n",
      "A:17, B:33, C:19091, D:24927\n",
      "A:33, B:17, C:24927, D:19091\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    tf_idf = TF_IDF_LR(TRAIN_FILE, 'utf-8')\n",
    "    tf_idf.get_data()\n",
    "    tf_idf.dump_features(FEATURE_FILE, 15000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T13:16:11.163918Z",
     "start_time": "2019-09-05T13:16:06.175633Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "由此可知语料库汇总词的总数量就是词袋的向量长度，且每一个词汇对应着它出现的顺序和频率\n",
      "feature value:  [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "tf_idf = TF_IDF_LR(TRAIN_FILE, 'utf-8')\n",
    "tf_idf.get_data()\n",
    "vectorizer = CountVectorizer(analyzer=\"word\",\n",
    "                     tokenizer=None,\n",
    "                     preprocessor=None,\n",
    "                     stop_words=None,\n",
    "                     max_features=1000)\n",
    "\n",
    "features = vectorizer.fit_transform(tf_idf.X)\n",
    "\n",
    "print(\"由此可知语料库汇总词的总数量就是词袋的向量长度，且每一个词汇对应着它出现的顺序和频率\")\n",
    "print('feature value: ', features.toarray())\n",
    "#print('feature name: ', features.get_feature_names())\n",
    "features.f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T21:43:07.844215Z",
     "start_time": "2019-09-05T21:43:07.828615Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0523087001575904e-05\n",
      "4.0523087001575904e-05\n"
     ]
    }
   ],
   "source": [
    "A=17\n",
    "B=33\n",
    "C=19091\n",
    "D=24927\n",
    "print((1 * (A * D - B * C) * (A * D - B * C)) / ((A + B) * (A + C) * (B + D) * (C + D)))\n",
    "\n",
    "A=33\n",
    "B=17\n",
    "C=24927\n",
    "D=19091\n",
    "print((1 * (A * D - B * C) * (A * D - B * C)) / ((A + B) * (A + C) * (B + D) * (C + D)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "data = pd.read_csv('./data/train_data.csv', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
